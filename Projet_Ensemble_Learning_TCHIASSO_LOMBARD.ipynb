{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de données\n",
    "\n",
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('beer_quality.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données en features et label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  \n",
       "0         9.4  \n",
       "1         9.8  \n",
       "2         9.8  \n",
       "3         9.8  \n",
       "4         9.4  \n",
       "...       ...  \n",
       "1594     10.5  \n",
       "1595     11.2  \n",
       "1596     11.0  \n",
       "1597     10.2  \n",
       "1598     11.0  \n",
       "\n",
       "[1599 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       5\n",
       "3       6\n",
       "4       5\n",
       "       ..\n",
       "1594    5\n",
       "1595    6\n",
       "1596    6\n",
       "1597    5\n",
       "1598    6\n",
       "Name: quality, Length: 1599, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données en train et en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Classification Binaire \n",
    "\n",
    "## Création de nouvelle variable en fonction de la médiane de y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "#Calcul de la médiane\n",
    "statistics.median(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implémentation de ybin en fonction de la médiane \n",
    "\n",
    "ybin = []\n",
    "\n",
    "for i in y:\n",
    "    m = 6\n",
    "    if i < m:\n",
    "        ybin.append(0)\n",
    "        \n",
    "    else:\n",
    "        ybin.append(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df.assign(ybin= ybin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>ybin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  ybin  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     1  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "1594     10.5        5     0  \n",
       "1595     11.2        6     1  \n",
       "1596     11.0        6     1  \n",
       "1597     10.2        5     0  \n",
       "1598     11.0        6     1  \n",
       "\n",
       "[1599 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>ybin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  ybin  \n",
       "0         9.4     0  \n",
       "1         9.8     0  \n",
       "2         9.8     0  \n",
       "3         9.8     1  \n",
       "4         9.4     0  \n",
       "...       ...   ...  \n",
       "1594     10.5     0  \n",
       "1595     11.2     1  \n",
       "1596     11.0     1  \n",
       "1597     10.2     0  \n",
       "1598     11.0     1  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On cherche à prédire ybin, on supprime quality\n",
    "new_df.drop(\"quality\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.iloc[:, :11]\n",
    "y = new_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc=StandardScaler()\n",
    "\n",
    "#scaler = sc.fit(X)\n",
    "#X = scaler.transform(X)\n",
    "\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth' : np.arange(1,5),\n",
    "             'min_samples_split' : np.arange (1,5)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf, param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 907, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 241, in fit\n",
      "    % self.min_samples_split)\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:925: UserWarning: One or more of the test scores are non-finite: [       nan 0.69790599 0.69790599 0.69790599        nan 0.70507287\n",
      " 0.70507287 0.70507287        nan 0.68365231 0.68365231 0.68365231\n",
      "        nan 0.73456919 0.73456919 0.73456919]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(criterion='entropy'),\n",
       "             param_grid={'max_depth': array([1, 2, 3, 4]),\n",
       "                         'min_samples_split': array([1, 2, 3, 4])})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6770833333333334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 1\n",
    "model_a = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   n_estimators=100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import pylab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_estimators = [10,20, 30, 40, 50, 60,70,80,90,100]\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=n_estimators[i])\n",
    "\n",
    "    # Train Adaboost Classifer\n",
    "    model = bdt.fit(X_train, y_train)\n",
    "    accuracy_train.append(bdt.score(X_train, y_train))\n",
    "    accuracy_test.append(bdt.score(X_test, y_test))\n",
    "\n",
    "\n",
    "pylab.plot(n_estimators, accuracy_train, '-b', label='acc_train')\n",
    "pylab.plot(n_estimators, accuracy_test, '-r', label='acc_test')\n",
    "pylab.legend(loc='upper left')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_estimators = [10,20, 30, 40, 50, 60,70,80,90,100]\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), algorithm=\"SAMME\", n_estimators=n_estimators[i])\n",
    "\n",
    "    # Train Adaboost Classifer\n",
    "    model = bdt.fit(X_train, y_train)\n",
    "    accuracy_train.append(bdt.score(X_train, y_train))\n",
    "    accuracy_test.append(bdt.score(X_test, y_test))\n",
    "\n",
    "\n",
    "pylab.plot(n_estimators, accuracy_train, '-b', label='acc_train')\n",
    "pylab.plot(n_estimators, accuracy_test, '-r', label='acc_test')\n",
    "pylab.legend(loc='upper left')\n",
    "pylab.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance des features selectionnées par AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure()\n",
    "    plt.barh(range(n_features),sorted(model.feature_importances_), align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns) \n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Features importance\", fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Classification multiclasse\n",
    "\n",
    "## 1. Création de ymulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On repart du dataset initial afin d'attribuer les 3 nouvelles modalités à partir de quality\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :11]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur l'application de la valeur des modalités : \n",
    "\n",
    "- Inférieur à 5 : 0\n",
    "- Egale à 5 : 1\n",
    "- Supérieur à 5 : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implémentation de ymulti en fonction de la moyenne\n",
    "\n",
    "ymulti = []\n",
    "\n",
    "for i in y:\n",
    "    \n",
    "    if i < 5:\n",
    "        ymulti.append(0)\n",
    "        \n",
    "    elif i == 5:\n",
    "        ymulti.append(1)\n",
    "    else:\n",
    "        ymulti.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ymulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1=df.assign(ymulti= ymulti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>ymulti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  ymulti  \n",
       "0         9.4        5       1  \n",
       "1         9.8        5       1  \n",
       "2         9.8        5       1  \n",
       "3         9.8        6       2  \n",
       "4         9.4        5       1  \n",
       "...       ...      ...     ...  \n",
       "1594     10.5        5       1  \n",
       "1595     11.2        6       2  \n",
       "1596     11.0        6       2  \n",
       "1597     10.2        5       1  \n",
       "1598     11.0        6       2  \n",
       "\n",
       "[1599 rows x 13 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>ymulti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  ymulti  \n",
       "0         9.4       1  \n",
       "1         9.8       1  \n",
       "2         9.8       1  \n",
       "3         9.8       2  \n",
       "4         9.4       1  \n",
       "...       ...     ...  \n",
       "1594     10.5       1  \n",
       "1595     11.2       2  \n",
       "1596     11.0       2  \n",
       "1597     10.2       1  \n",
       "1598     11.0       2  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On cherche à prédire ymulti, on supprime quality\n",
    "new_df1.drop(\"quality\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df1.iloc[:, :11]\n",
    "y = new_df1.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre d'occurence par modalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    855\n",
       "1    681\n",
       "0     63\n",
       "Name: ymulti, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1['ymulti'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    588\n",
       "1    486\n",
       "0     45\n",
       "Name: ymulti, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 588, 1: 588, 0: 588})\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE(random_state=0, sampling_strategy='all')\n",
    "X_train_balanced, y_train_balanced = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_balanced)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val, y_train1, y_val = train_test_split(X_train_balanced, y_train_balanced, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_val1, y_train2, y_val1 = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats sur la base avec les données équilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3452830188679245"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=1, early_stopping = True, max_iter=300,learning_rate_init=0.1)\n",
    "clf.fit(X_train1, y_train1)\n",
    "\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'activation' : ['identity','logistic', 'tahn', 'relu'],\n",
    "              'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "              'learning_rate' : ['constant', 'invscaling', 'adaptive']\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gride = GridSearchCV(clf, param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:925: UserWarning: One or more of the test scores are non-finite: [0.53231296 0.33954445 0.43687173 0.55342813 0.37436227 0.44810572\n",
      " 0.47900332 0.41814292 0.46924723 0.4935486  0.36545538 0.37760113\n",
      " 0.33954445 0.33549258 0.40690234 0.39217603 0.35492907 0.38731444\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.33954445 0.35735822 0.35735822\n",
      " 0.33954445 0.33387314 0.33549587 0.35498502 0.33387314 0.33306672]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=MLPClassifier(early_stopping=True, hidden_layer_sizes=1,\n",
       "                                     learning_rate_init=0.1, max_iter=300),\n",
       "             param_grid={'activation': ['identity', 'logistic', 'tahn', 'relu'],\n",
       "                         'learning_rate': ['constant', 'invscaling',\n",
       "                                           'adaptive'],\n",
       "                         'solver': ['lbfgs', 'sgd', 'adam']})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gride.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', early_stopping=True, hidden_layer_sizes=1,\n",
       "              learning_rate='invscaling', learning_rate_init=0.1, max_iter=300,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gride.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  gride.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', early_stopping=True, hidden_layer_sizes=1,\n",
       "              learning_rate='invscaling', learning_rate_init=0.1, max_iter=300,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41132075471698115"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0,\n",
       "       0, 2, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 2, 2, 0,\n",
       "       2, 1, 1, 2, 2, 2, 2, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 2,\n",
       "       2, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 0, 2, 0, 0, 0, 2, 2, 1, 1, 0,\n",
       "       1, 2, 1, 0, 1, 1, 0, 2, 2, 2, 2, 0, 1, 1, 2, 1, 0, 0, 2, 0, 0, 0,\n",
       "       2, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 2, 2, 0, 2,\n",
       "       0, 2, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 2, 0, 2,\n",
       "       2, 0, 2, 2, 1, 0, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 0, 0, 1, 0, 2, 0,\n",
       "       2, 1, 2, 2, 1, 1, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0,\n",
       "       1, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 2, 1, 0, 1, 0, 2, 1, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2,\n",
       "       2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 1, 1, 2, 1,\n",
       "       0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 2, 1,\n",
       "       1, 1, 2, 1, 2, 2, 1, 0, 1, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 0, 1, 1,\n",
       "       0, 2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 1, 2, 0, 1, 1, 2, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0,\n",
       "       0, 0, 2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[93, 43, 42],\n",
       "       [52, 72, 59],\n",
       "       [74, 42, 53]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats sur la base avec les données non-équilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5327380952380952"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=1, early_stopping = True, max_iter=300,learning_rate_init=0.1)\n",
    "clf.fit(X_train2, y_train2)\n",
    "\n",
    "clf.score(X_val1, y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 673, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 357, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 450, in _validate_hyperparameters\n",
      "    % (self.activation, list(sorted(ACTIVATIONS))))\n",
      "ValueError: The activation 'tahn' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:925: UserWarning: One or more of the test scores are non-finite: [0.70100441 0.52246448 0.5144782  0.6578148  0.51587457 0.57488976\n",
      " 0.68323534 0.57095378 0.57995264 0.60642659 0.51734444 0.54542708\n",
      " 0.56821003 0.52235016 0.52235016 0.63488486 0.50311939 0.51980238\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.52235016 0.52235016 0.53773477\n",
      " 0.53381512 0.50568349 0.50451576 0.52235016 0.52363221 0.52235016]\n",
      "  category=UserWarning\n",
      "C:\\Users\\Rudyl\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', early_stopping=True, hidden_layer_sizes=1,\n",
       "              learning_rate_init=0.1, max_iter=300, solver='lbfgs')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {'activation' : ['identity','logistic', 'tahn', 'relu'],\n",
    "              'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "              'learning_rate' : ['constant', 'invscaling', 'adaptive']\n",
    "             }\n",
    "\n",
    "gride = GridSearchCV(clf, param_grid, cv = 5)\n",
    "\n",
    "gride.fit(X_train2, y_train2)\n",
    "\n",
    "model =  gride.best_estimator_\n",
    "model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7261904761904762"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_val1, y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict(X_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   5,   3],\n",
       "       [  0, 100,  49],\n",
       "       [  0,  35, 144]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    179\n",
       "1    149\n",
       "0      8\n",
       "Name: ymulti, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "### Données équilibrées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "n_estimators = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    bagging = BaggingClassifier(base_estimator=MLPClassifier(early_stopping=True, hidden_layer_sizes=1))\n",
    "    model = bagging.fit(X_train1, y_train1)\n",
    "    accuracy_train.append(bagging.score(X_train1, y_train1))\n",
    "    accuracy_test.append(bagging.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2deZzN9ffHX+/ZzNi3QYwspX6MZXAtpaRElrJcRVGRIkKEryz5tviKtKkoIZEYiRQKIUouMva9LGHGNvZtjJm55/fHuddcM/feucvnc7c5z8djHnPvZ3m/z+cur/v+nPd5n6OICIIgCELoEuZvAwRBEAR9EaEXBEEIcUToBUEQQhwRekEQhBBHhF4QBCHEifC3ATkpXbo0Va5c2d9mCIIgBBVbtmw5S0Sx9vYFnNBXrlwZSUlJ/jZDEAQhqFBKHXW0T1w3giAIIY4IvSAIQogjQi8IghDiBJyP3h4ZGRlITk7GjRs3/G1K0BIdHY24uDhERkb62xRBEHxMUAh9cnIyihQpgsqVK0Mp5W9zgg4iwrlz55CcnIwqVar42xxBEHxMULhubty4gVKlSonIe4hSCqVKlZI7IkHIpwSF0AMQkfcSef0EIf8SNEKf37l0Cbh+3d9WCIIQjIjQBwFZWcChQ/xnNvvbGkEQgg0Rej/z7rvv5nnMxYss8OnpwOnTvO2ll17C3r17dbZOEIRQQITezzgSeiKC2TJ8P38eiIoCihcHTp4Ebt4Epk+fjho1avjSVEEQgpSgCK+0ZdAgYPt2bdtMSAAmTsz7uA4dOuD48eO4ceMGBg4ciN69e2P58uUYOXIksrKyULp0aaxevRpXr17FgAEDkJSUBKUU3nzzTXTq1ClXe8OHD0daWhoSEhIQHx+PsWPHonXr1nj44YexYcMG/Pjjj3j33fFYt24zMjPT8NRTT8JofBvJyUDPns3wwQcfwGAwoHDhwhg4cCCWLl2KmJgY/PTTTyhbtqy2L5IgCEGLjOjdYMaMGdiyZQuSkpLw6aef4vTp0+jVqxcWLlyIHTt24PvvvwcAjBkzBsWKFcOuXbuwc+dOPPLII3bbGz9+PGJiYrB9+3bMmTMHAHDgwAE8//zz2LZtGypVqoQhQ8bim2+SkJS0E+vX/47z53fi/Hn221u5du0aGjdujB07dqBp06aYNm2a7q+FIAjBQ9CN6F0ZeevFp59+ikWLFgEAjh8/jqlTp6Jp06a3FiGVLFkSALBq1SrMmzfv1nklSpRwuY9KlSqhcePGt54nJs7Hd99NRUREJk6ePInU1L0oU6Y20tMBa133qKgoPP744wCA+vXrY+XKlV5dpyAIoYWM6F1k7dq1WLVqFTZs2IAdO3agbt26qFOnjt34dCLyOG69UKFCtx4fOHAE06d/gAULVmPnzp1o27Ytbt68gbg4HtFfusTHRUZG3uovPDwcmZmZHvUtCEJoIkLvIpcuXUKJEiVQsGBB7N+/Hxs3bkR6ejp+//13HDlyBABw/vx5AEDLli0xadKkW+deuHDBYbuRkZHIyMiwuy8l5TJiYgqhcuViOH36NJYtWwYAKFkSCA8Hzpy53YUjCIJgDxF6F2nVqhUyMzNRu3ZtjB49Go0bN0ZsbCymTp0Ko9GIOnXqoEuXLgCAN954AxcuXEDNmjVRp04drFmzxmG7vXv3Ru3atdGtW7dc++64ow5q1KiL+vXj0bNnTzRp0gQAoBRQoACQmclROIIgCM5QZHX0BggGg4FyVpjat28fqlev7ieL/MONG8Du3UBcHFCunP1jjhzh0Mv4eCA6Ou828+PrKAj5BaXUFiIy2NsnI/oAxeIFgrN53Lg4Ht0fP+4bmwRBCE6CLuomWGnUqBHS09Nv2zZ79mzUqlUr17FELPSFC7OLxhGRkUD58kByMk/MFiumtdWCIIQCIvQ+YtOmTS4fm5bGrps778z72DJlgNRUHtUXKQKEyT2aIAg5EFkIQFxx21gJCwMqVuQfhtRUfe0SBCE4cUnolVKtlFIHlFIHlVLDnRz3pFKKlFIGm20jLOcdUEo9poXRoYzVbVO0KLtmXKFYMT7+xAnAQaSmIAj5mDyFXikVDmAygNYAagB4RimVK5uWUqoIgFcBbLLZVgPA0wDiAbQC8LmlPcEB165x0jLLIluXUIpH9VlZLPaCIAi2uDKibwjgIBEdJqKbAOYBaG/nuDEAJgCwrVfXHsA8IkonoiMADlraEyzkzF55/jwLtytum5kzZ+KERdljYrL99VKgRBAEW1wR+goAbAP4ki3bbqGUqgugIhEtdfdcy/m9lVJJSqmk1HzmaLYVeqvbplgxXvmaF7ZCD3AETkQEcOxYdh4cQRAEV4TeXtKWWzKilAoD8DGAIe6ee2sD0VQiMhCRITY21gWT/EOHDh1Qv359xMfHY+rUqQCA5cuXo169eqhTpw6aN28OALh69SpeeOEF1KpVC7Vr18bChQvttmebprhbt264cgVYvPhbdO7cEAkJCXj55ZeRlZWFrKws9OjRAzVr1kStWrXw8ccfY8GCBUhKSkK3bt2QkJCAtLQ0REQAFSoAV68CTrIuCIKQz3AlvDIZQEWb53EAbD3BRQDUBLDWklirHIDFSql2LpzrPn5MSD9jxgyULFkSaWlpaNCgAdq3b49evXrhjz/+QJUqVW7lurFNUww4znUzfvx4TJo0Cdst17Nq1T6sWvUdNmxYjwIFIvHKK69gzpw5iI+PR0pKCnbv3g0AuHjxIooXL45JkybdyklvpXRpzoGTnOz6nYEgCKGNK0K/GUA1pVQVACngydWu1p1EdAlAaetzpdRaAEOJKEkplQZgrlLqIwDlAVQD8Jd25vsWPdMUm83AypWrceDAFjRq1AAAkJaWhjJlyuCJJ57A4cOHMWDAALRt2xYtW7Z02I5SHH9/4ACXHSxf3uPLFQQhRMhT6IkoUynVH8AKAOEAZhDRHqXUOwCSiGixk3P3KKXmA9gLIBNAPyLyLt+inxLS26YpLliwIJo1a4Y6dergwIEDuY71JE3xpUuA2Uzo1q07PvpoXK79O3bswIoVKzB58mTMnz8fM2bMcNhWkSI8mXvqFFCqlPPVtYIghD4uxdET0S9EdA8R3UVEYy3b/mtP5ImoGREl2TwfaznvXiJapp3pvkXvNMXnzwONGzfHkiULcObMmVvtHT16FGfPnoXZbEanTp0wZswYbN26FQBQpEgRXLlyxW67cXE8IZuSosnlC4IQxMjKWBfRO01x//7dYDDUwP/+9z+0bNkStWvXRosWLXDy5EmkpKSgWbNmSEhIQI8ePTBuHI/4e/TogT59+tyajLWlQAHOenn+PODgt0AQhHyCpCkOAM6d45TD997LbhetyMoC9uzhkMvq1YH9+0P7dRSE/IykKQ5wzp8HoqI4W6WWhIezC+f6deDsWW3bFgQheJDslT7CUZri6tVr4fJlXtXqYZlZp5QowT8gKSmu584RBCG0EKH3EY7SFKem8qSpO7lt3MGaB2ffPkmNIGjP2bNc3Uzru1FBW4LGdRNocwlacf48f1EKFtSvj0KFgFKlCJcuAfv369ePkP946CFgwAB/WyHkRVAIfXR0NM6dOxdyYn/zJkfElCypj9vGChGhYMFzOHIkGoMH69ePkL84eRLYuxdYt87flgh5ERSum7i4OCQnJyPUEp5dvsw5aaKieMGUnkRHRyMrKw7LlgE//wy0batvf0Los2ED/z90iD/HrmRcFfxDUAh9ZGTkrTQDoUSDBhwCaVn/pDsvvwxMngy89hrQogX/wAiCp5hM2Y+3bAEefdR/tgjOCQrXTSjyzz9AUhLwzDMunrB5M/Dvv171GRUFfPwx9/3ZZ141JQgwmYD4eH6cY+mLEGCI0PsJa84zy2Ja56Sm8qzXgw/y6iovaN2a3TbvvMNJzwTBE27c4FF8mzbA3XeL0Ac6IvR+gAhITGTdvvNOF06YOJG/WadPAz16eF1V5KOPONRy1CivmhHyMVu3cjDB/fcDBgPfcAqBiwi9H9i5k+PaXXLbXLwITJoEdOoEfPghsHQpK7UX3HMPMHAgMGMGj8oEwV3Wr+f/VqE/dozrIAiBiQi9H0hM5PQETz7pwsGTJ3N4zsiRQP/+gNEIDB8ObNzolQ2jRwOxscCrr0rZQcF9TCZ22ZQpw0IPyKAhkBGh9zFE7J9v0YKF1inXrvHsaZs2QN26HGz/1VecwKZLF15t5SHFigHvvstf2MREj5sR8iFE/Lm5/35+bv1oip8+cBGh9zEbNgBHj7rotpk6lSdfbZ3pxYsD8+fzapUXXvBqOP7CC0D9+sCwYfybIgiucPgwu2msQl+0KGdeFaEPXETofUxiIqc86NAhjwPT04EPPgCaNcv+Rllp0ACYMAFYvBj45BOPbQkL49NTUoDx4z1uRshnWOPnbT+WBoMIfSAjQu9DMjN5MN62LY+CnDJzJnDihOPQmIEDgfbteTj+l+dleJs0Abp2Bd5/3+swfSGfYDLx57dGjextBgN/XE+c8J9dgmNE6H3ImjV8y5un2yYzE3jvPaBhQ6B5c/vHKMVhM+XLs7/+4kWP7XrvPZ4c/s9/PG5CyEeYTEDjxvyZsSITsoGNCL0PSUzkClJt2rhw4JEjPJp3lu2sZEngu++A5GSgZ0+P/fVxcRzIs2AB/xgJgiMuXwZ27crtTUxIYFeguG8CExF6H5GeDvzwA9CxIxAT4+RAsxkYNw6oVQt4/PG8G27UiB3sixZxvL2HDB0KVKoEDBrENxSCYI9Nm3g8kVPoCxViV44IfWASWkK/f3/ABoUvW8YZKvN02yxaxKupRo7kIZIrDB7MPwpDhnj8TYuJ4bnfnTuBadM8akLIB5hMfJPZqFHufdYJ2QD9CuZrQkfof/uNhxQ//OBvS+ySmAiULu3Y5Q6AvyFjxwLVqgFPPeV640rx5G25cuyv9zDncadOHOQzerRXIfpCCGMy8c2mvWACg4HnoJKTfW+X4JzQEfqmTYHatTka5coVf1tzG1evAkuWsHY7rdu6fDmwbRs7zG1nulyhVCleiXX0KPDSSx4Nq5TitDoXLgBvveX26UKIk5XFC7Jzum2sWCdkxX0TeISO0EdEAF98wUHhb7/tb2tu46efgLS0PNw21tF8xYrAs8961tH99/Ny1wUL+LXwgDp1gN69gc8/Bw4c8MwMITTZs4cnY5s0sb+/Th3+GorQBx6hI/QAcN99PJqdOJFDAwKExESObHH0BQEA/PEHZ4oaNsy7iiBDh3Iu4tde87iiiXU0/803npshhB72FkrZEh3Nbh0R+sAjtIQe4AiU4sWBvn05gsXPnDsHrFgBPP10HnOrY8cCZcsCL77oXYdhYazQsbFA5848BHOTsmWBhx8Gvv9eJtaEbEwm/mw4K/YmE7KBSegJfalSnB5g/Xpg1ix/W4MFCzhcsWtXJwdt3gysXMnRM05jL12kdGn21//7L/thPPjWPfUUV6LaudN7c4TQwJrIzNnSDoOBJ/JllXVgEXpCD3BxjiZNeKmnlxWZvCUxkRM+JSQ4OWjsWK6s3Levdh0/8AAwZgwvqJo61e3TO3bk+eD587UzSQheTp/mIuCO3DZWZEI2MAlNoQ8L49nEixeBESP8ZkZKCrven3nGySho1y6erX31VV42qyWvvw489hhHIm3f7tapsbHivhGy2bCB/+cl9DVr8hSTCH1gEVJCf5sg1a7NyzynTcv+lPqY775jm5xG24wbBxQuzEKvNVZ/falS7K93M+xU3DeCFZOJBbxePefHRUVx9I2UFgwsQkbok5M56OY2UXrzTaBCBXaJ+GFdf2IifzHuucfBAQcP8q9B376ct0YPypRhQw4dAvr0cWt4bnXffP+9PqYJwYPJxLULoqPzPtZg4ORmARALIVgIGaG/fJnF/sEHbRJzFSnCCdd37PAqD4wn/PMP3746Hc2PH88rqAYP1teYpk15bcHcuVyhykViY3ml7Pz54r7Jz6Sn82c5L7eNFYOBv48HD+prl+A6ISP0NWqwhyYuDmjVigfKALjGaqtWvK4/JcVn9sybx/+7dHFwwPHj7FZ56SVOXaA3I0YAjz4KDBjgli+mc2dx3+R3tm1jsXdH6AHx0wcSLgm9UqqVUuqAUuqgUmq4nf19lFK7lFLblVJ/KqVqWLZHKqVmWfbtU0rpOjNasSLw55+ccOnpp3ndFJTi0XxGhv4jZwtE7C158EG2yS7vv88HDhvmE5sQHg58+y2vMejcmfMyuIC4b4S8FkrlpEYNdvGI0AcQROT0D0A4gEMAqgKIArADQI0cxxS1edwOwHLL464A5lkeFwTwL4DKzvqrX78+eUtaGpHRSAQQDRlClJVFRG+/zRtWrPC6/bzYvp27+vxzBwecPk0UHU30wgu625KL334jCgsjeu45IrPZpVOaNyeqVs3lw4UQw2gkqlrVvXPuu4/owQf1sUewD4AkcqCrrozoGwI4SESHiegmgHkA2uf4sbBdflkIgNWjSwAKKaUiAMQAuAnA/aWabhIdzX7lfv2ADz8EnnsOuDloGGeF7NcPuHFD1/4TE3kU/OSTDg74+GPg5k1OXuZrHn4Y+O9/gdmzOeOlC0j0Tf6FKHuhlDsYDJyBIytLH7sE93BF6CsAOG7zPNmy7TaUUv2UUocATABgjRVcAOAagJMAjgH4gIhyJcBVSvVWSiUppZJSU1PdvAT7hIcDn33GOb7mzgXadorGtQmTeYZowgRN+rAHEfvnW7TgycxcXLgATJ7M6ukwHEdn3ngDeOQR/tHbsyfPw41GjtQU903+499/gVOnPBP6a9ckMV6g4IrQ21vqkysGg4gmE9FdAF4H8IZlc0MAWQDKA6gCYIhSqqqdc6cSkYGIDLF21dEzlOI5yJkzORLngbdbIK1dF1Z/nUICNmzgTMEOo20mTeJ49pEjdenfJcLDgTlzOKn4U0/xN9IJsngq/+Kuf96KTMgGFq4IfTIA2ynFOADOar3PA9DB8rgr2F+fQURnAKwHYPDEUG/o3h1YupTdD822foSsiCigf39dVCsxkV1HHTrY2Xn1Ks8QP/EEL+jyJ+XKsdjv38+vRR489RTw99/ivslvmEy8nq9mTffOu/deLi8oQh8YuCL0mwFUU0pVUUpFAXgawGLbA5RS1WyetgXwj+XxMQCPKKYQgMYA9ntvtvu0agWsXQscSS+P0TSGU0ouXKhpH5mZPDfQtq39Cjz48kvO+OTP0bwtzZuzG2fmzDwTwIn7Jn9iMgGNG7tfByc8nBcLitAHBnkKPRFlAugPYAWAfQDmE9EepdQ7Sql2lsP6K6X2KKW2AxgMoLtl+2QAhQHsBv9gfE1EfhsTGgz8wV1Yrh92qASk9RmkaTWqNWu4lJpdt82NG1yU9ZFH+JsTKLz5JvDQQ8ArrwB79zo8TNw3+Y8rV/gOzl23jRWDgWPwpdh8AOAoHMdff1qEV+bF6dNEPf5vAxFAO1oO0azdF14gKlqU6Pp1Ozs//5xjLlev1qw/zUhJIYqNJYqPJ7p2zeFhU6bwJWzf7kPbBL+xahW/38uXe3b+nDl8/o4d2tol2AdehleGHGXKAJ9tboxf4nqhxq8T8UXfnV6PUm/c4LrkHTvaSSmfkQG89x6P5B9+2LuO9KB8eV5MtXev0+RqHTuK+yY/YTJxQEOjRp6dLxOygUO+FHqAJ5habB6H6wVKoPaUvujT2+zVLeayZcClSw7cNnPncijOqFHOqzb4k5YtOUTpq69Y9O1Qpoy4b/ITJhMQH8+LqT3h7rt5rkqE3v/kW6EHgMhypVDk8wloAhMyps+E0Qhcv+5ZW4mJ7Mdu3jzHjqwsTkVcpw7P0gYyb7/NeRv69OFoHDtYo28CqCSvoANmM4cKe+qfB/juz1paUPAv+VroAUD16A488AAmFxoG05JzaN4cOHvWvTauXAGWLGERjIjIsfOHH3jVyMiRgTuatxIRwb9YMTGcDyctLdchVveNVJ4Kbfbt4ztUb4QeYKHfsYMXggv+I98LvbUaVcyNi9jcfDi2beMqfO7UvPzpJ/bR53LbEHGZwHvvBTp10tJq/ahQAfj6ax6yJybm2l2mDKcuFvdNaLN+Pf9v0sS7dgwGFvndu723SfAcEXoAqFULeO01VFk9HRs/3oDTp7mIiavV9xITOUtlrtHPL7/wcGb4cPcDkf1J27ZA5cp8N2KHzp3FfRPqmEzsirzrLu/akQnZwECE3sqbbwJxcUj4si/+XJuJiAiu17F6tfPTzp0Dfv2V0yKH2b6a1tF8pUpAt266mq45SvEKqZUruYJEDiT6JvSxJjLz1ttYuTIXTxOh9y8i9FYKF75VjSp+zSRs2MAa3bq1XQ/GLRYs4AUhudw2a9fybNawYVxFKtgwGvme+5dfcu2yum+k8lRokprK6UK89c8D/ENhMEgNWX8jQm9Lx46s7KNHI06lYN06duF07crpju2RmMgu+ISEHDvGjuV8Mj176m62Ltx3H9sv7pt8x4YN/F8LoQdY6Hfvtju3L/gIEXpbrNWoMjOBwYNRvDinxOnUCRg6lAtU2RY8TkkB/viDR/O33eJu2sQ+nyFDXKumHIiEhXFmtl9+cRp9I+6b0MNk4pvQ+vW1ac9g4K+UJMTzHyL0OalalUMh588Hfv0V0dFcf7Z/f64X0q0b188EeDuRHbfN2LHsmOzTx+fma4rRyCmMV67MtUvcN6GLycQJyXKt8PYQmZD1PyL09hh2ezWq8HDg00+B8eO5qEjr1hxjnJjIX4jb6ofs3MlB9QMHst8/mGnWjJdFOnDfyOKp0OPmTfana+W2AYC4OB4YiND7DxF6exQoAHz+ORcnee89AOyaef114JtvgHXrOP9HUpKd0fy77wJFigADBvjebq2JjATatQMWL+Z8PTmQ1MWhx/btvCZES6G3TsiK0PsPEXpHPPoox0yOG3dbNarnnuMiJsnJ/LxLF5tz/v6bfRmvvAKUKOFbe/XCaOTyh7//nmuXLJ4KPTytKJUXBgPnzMujmJmgEyL0zvjwQyAqdzWqxx4DNm7kuiUVbWtvjR/PdwOvveZ7W/WiZUugYEGn7psDB8R9EyqYTBxWXL68tu0aDBzI4OoiREFbROidUb488L//2a1GVbMmD3ZvcfQoMHs20KsXULasb+3Uk5gYoE0bYNGi20OOLIj7JnQg4tQH3qY9sIc1gkfcN/5BhD4vXnmFg+QH5VGN6v332Rn5n//4zjZfYTQCp07xbUwOxH0TOhw7Bpw4ob3bBuAxU/nyIvT+QoQ+LyIigClT+Bvw1lv2jzl1Cpg+HXj++Ry+nBChbVt2YYn7JqTRyz9vRSZk/YcIvSs0asQumU8+sb/q4+OPOSpl+HDf2+YLihblyekffrA7bBf3TWhgMgGFCnGOPz0wGHhAYCd9kqAzIvSuMm4cR9L07Xu7r/r8eQ7F7NKFS+qEKkYjcOQIZ+PMgbhvQgOTicc0uWoqaITBwJ+Pbdv0aV9wjAi9q5QsyX54kwmYOTN7+2efAVev8mraUKZdOx625+G+kbzjwcnVq/wbrpfbBpAJWX8iQu8O3btzqb1hwzg/8ZUr7M5p357DcEKZ2FjO2+xA6K3uG6k8FZxs3sxVL/UU+jJlgDvvFKH3ByL07qAUu2kuXWJ//JQpvJho1Ch/W+YbjEZgzx4euuegTBngoYfEfROsWCdiGzfWtx+ZkPUPIvTuUrMmL4iaPp2Tl7VoATRo4G+rfEOHDvx/0SK7uzt3FvdNsGIyATVq6L+gu0EDXmh+4YK+/Qi3I0LvCf/9L4dRXrqUf0bzAF9zw4Z5um8k+ia4MJs5B72ebhsr1kyWW7fq35eQjQi9JxQuzGo2fjz7rfMTRiM7dI8dy7XL6r6R1MXBxYEDPML2hdDLhKx/EKH3lEaNOJ2lt0U1g42OHfn/jz/a3S3RN8GH3gulbClRgguOi9D7FhF6wT3uuYfnKcR9EzKsXw+UKpWjroKOSA1Z3yNCL7iP0chJ+c+cybWrbFlx3wQbJhOP5n11c2owcA7A1FTf9CeI0Aue0LEjz+AtXmx3t7hvgoezZ/m98oXbxop1QnbLFt/1md8RoRfcp04doEoVcd+EANaEpL4U+nr1+L/46X2HCL3gPkqxmq9axSGmObC6b2TxVOBjMnFuG+so2xcULQrce68IvS8RoRc8w2jkjJ0//2x391NPAfv3i/sm0DGZgLp1uYiYL5EVsr5FhF7wjMaNgXLlxH0TxGRkAH/95Vu3jRWDAUhJAU6e9H3f+REResEzwsJ4UnbZMuD69Vy7xX0T+OzYAaSl+U/oAZmQ9RUuCb1SqpVS6oBS6qBSKld1DaVUH6XULqXUdqXUn0qpGjb7aiulNiil9liOidbyAgQ/YjSyyP/6q93dVvfNnj0+tktwCV8ulMpJQgKPFcR94xvyFHqlVDiAyQBaA6gB4BlbIbcwl4hqEVECgAkAPrKcGwHgWwB9iCgeQDMAGdqZL/iVhx7ipY6SujgoMZk4fVFcnO/7LlwYqF5dhN5XuDKibwjgIBEdJqKbAOYBaG97ABHZFgcrBMB6s94SwE4i2mE57hwRZXlvthAQREZyQZIlS9jhm4OyZTkVkLhvAhPrQil/YZ2Qlc+G/rgi9BUAHLd5nmzZdhtKqX5KqUPgEf2rls33ACCl1Aql1Fal1DB7HSileiulkpRSSamyXC64MBqBixeBtWvt7u7cWdw3gcjx4/zXpIn/bDAYgNOneVJW0BdXhN7ewuhcv8FENJmI7gLwOoA3LJsjADwAoJvlf0elVHM7504lIgMRGWJjY102XggAWrTgitLivgkq/Omft2KdkBX3jf64IvTJACraPI8DcMLJ8fMAdLA593ciOktE1wH8AqCeJ4YKAUpMDNCmDRcjycrtlRP3TWBiMnHsfO3a/rOhTh0gPFyE3he4IvSbAVRTSlVRSkUBeBrAbUlOlFLVbJ62BfCP5fEKALWVUgUtE7MPAdjrvdlCQGE08j24dT19DsR9E3iYTFxDJjLSfzbExHAiVBF6/clT6IkoE0B/sGjvAzCfiPYopd5RSrWzHNbfEj65HcBgAN0t514AR+BsBrAdwFYisr+UUghe2rQBoqJk8VSQcO0asG2bf902VmRC1jcoCrBX2GAwUHwNIs8AACAASURBVJL8xAcfjz/OQ/bDh+3mu334YeDUKWDv3vxXqyXQ+P13oFkzYOlSoG1b/9oyZQrQty9w5AhQubJ/bQl2lFJbiMhu1iJZGStog9EI/PsvsH273d2yeCpwsE7ENm7sXzsALhYOiPtGb0ToBW1o1479M+K+CXhMJuD//o+rSvmbmjXZ6ydCry8i9II2lC7NK2UdCH25chx9I5Wn/AuR/xdK2VKgAEf+iNDriwi9oB1GIzvh9++3u1vcN/7n77+B8+cDR+iB7AlZs9nfloQuIvSCdnSwLJ9YtMjubqORJ2LFfeM/AmGhVE4MBq5fc+iQvy0JXUToBe2IiwMaNXLqvpHUxf7FZOI8dPfe629LspEVsvojQi9oi9HI39hjx+zufuopYN8+cd/4C6t/PiyAvvk1agDR0SL0ehJAb7cQEnTsyP/FfRNwnD/PUyiB5LYBeHVuQoIIvZ6I0AvaUq0aUKuWS+4bwbdYM1QEmtAD7L7ZutVuuiRBA0ToBe0xGoF16zj/jR3EfeMfTCZOImZdpBRIGAzA1ascFSRojwi9oD1GI8+2Ll7scLdSkrrY15hM7CIpVMjfluRGJmT1RYRe0J5atYC77spz8ZS4b3xHZiawaVNgum0AXqlbsKAIvV6I0AvaoxQP21ev5upTdujcWdw3vmTnTq7jHqhCHx4O1KsnQq8XIvSCPhiNXEf2Z/tZqcV941sCcaFUTgwGTp+cmelvS0IPEXpBHxo2BMqXF/dNgGAyARUqABUr5n2svzAYgLQ0vtMTtEWEXtCHsDCOqV+2jH0GdpDoG99hXSgVyLUAZEJWP0ToBf0wGnmItmKF3d2dOsniKV+QkgIcPRrYbhuAl2AUKSJCrwci9IJ+NG0KlCzpUupiQT82bOD/TZro0PibbwKffaZJU2FhQP36IvR6IEIv6EdEBNC+PbBkCXDzpt1DxH2jP+vXcyHuhASNGz58GHjnHeDVV4H339ekSYMB2LHD4cdF8BARekFfjEbOQbtmjd3d4r7RH5OJV8NGRmrc8Fdf8TC8bVtg2DBg8mSvmzQYgPR0+eHXGhF6QV8efRQoXDhP983s2exLFrQlLY1zyGjun8/IAGbMAFq35gR27dsD/fvzNi+QCVl9EKEX9CU6GmjTBvjxR4cZq4YNA06c4HS1U6ZIpSEtSUriuHTNhf7nn4FTp4BevfhW4bvvgMceA156CUhM9LjZqlU5X74IvbaI0Av6YzQCZ85kr9rJQZs2wO7d7F7o25ezW0ostTZYX/L77tO44WnTgDvuYLcNwMVff/iBb8+ee85hmuq8UIpH9Zs3a2irIEIv+IA2bYCoKIfuG4BT46xcCXz9NftnExJ4nk8m5bzDZALuuYdrt2vGsWPA8uVAz5484W6lYEGeeG/QAOjShddQeIDBAOzaBdy4oZG9ggi94AOKFAFatmShd1JDUCmgRw8ezRuNHLlXr152eKDgHkTZC6U0ZcYM9q+9+GLufUWKsMDXrMlvooNJeGcYDOxu2rlTA1sFACL0gq8wGnkkuHVrnoeWLctu3qVLgcuXOf57wADgyhUf2Kk16ekceugguZueHDwInD2rsdBnZbHQt2wJVKli/5jixYFff+XbtCeecOiyc0TATMhu3sy3mCGACL3gG554glMUOnHf5KRtW3bjDBjAkXs1arD4BxVTp/Js8wcf+LxrXRKZrVgBHD/Ok7DOKF0aWLWK8x21bg1s2eJyFxUrArGxfhR6s5l/nO+/n91T1tJcQYwIveAbSpfmWVY3hB5gT8Ann7BoFSvGvxddujgsXhVY3LwJTJjAj7/4Arh2zafdW1+z6tU1bHTaNFbhdu3yPrZcOU5VXbIk3wHs2uVSF9YJWb8I/dmz/CEbNoz/lyoFjB3rB0O0RYRe8B1GI7B/v0chNY0bs9dnzBiO1Kxene+qnbj8/c/s2UByMvDGG1yZe9Ysn3ZvMnG0TZhW3/KTJ3mytUcPnlx3hYoVWexjYnhNxYEDLp1mMPDdnIN8ePqwbh1HAaxaBUyaBCxcCAwcyLeRO3b40BAdIKKA+qtfvz4JIUpyMhFA9L//edXMvn1EDz7ITT3yCNE//2hkn5ZkZBDdfTdR/fpEZjNRw4b8PDPTJ91fuECkFNGYMRo2OnYsv+gHDrh/7v79RGXKEFWoQHToUJ6H//QTd7V+vQd2uktmJn8mw8L4Pdq6NXvf+fNERYoQde7sA0O8A0ASOdBVGdELvqNCBR6au+m+ycn//R+wdi0vrkpK4sqF773HizUDhu+/59nQkSPZFzFkCD/30STDxo18t6OZf95sBqZPB5o143hNd7n3Xh4pp6UBzZuzn98JPpuQPX0aaNWK77q6dOHbxrp1s/eXKAH068fvp4t3IwGJo18Af/3JiD7EmTCBh2pHjmjSXEoKUceO3GRCAlFSkibNekdWFlHNmkQ1avBjIh7hV6rEtyI+YPRoHqBeuaJRgytX8os8Z4537SQlERUtSlStGtHJk04PveMOouee8647p6xaRVS2LFF0NNG0aXznZY/Tp4liYoh69NDRGO+BjOiFgKFjR/7v4crJnFiLWC1cyIOzhg2BoUN9Pu95O0uW8FLfESOyHeQREcCgQewH9sGyT5MJqFOH0wxpwtSpPKlqNHrXTv36HGd/4gT77M+edXiobhOyWVm8SKNFC76mv/7i1A2OqrKUKcNRRt9+y4n9gxFHvwD++pMRfT6gdm2iBx7QvNkLF4hefpkHnlWqEK1YoXkXeWP1x1etyqN4Wy5d4tFsly66mpCRQVS4MFG/fho1eOYMUWQk0aBBGjVIRL/9xiPpunX5jbPD22/zPMPly9p1S8nJRA89xB+SHj2Irl517bzjx/k10OxF1R7IiF4IKIxGTpKucYxk8eLst//9dw4Keewx4PnnnQ4atWf1ah4hvv767ekBAKBoUaB3b2DBAl1Hhrt3A1evauifnzWLJ0Dyip13h4cf5lux3bs5zt7OajiDgecZtm3TqM/lyzmqZvNmvqavvwYKFXLt3Lg4oHt3nqc4dUojg3yIo18A2z8ArQAcAHAQwHA7+/sA2AVgO4A/AdTIsf9OAFcBDM2rLxnR5wN27uQR1Zdf6tZFWhrRG28QRUQQlS5N9O23jl2wmtKsGVH58kQ3btjff+wYGzV4sG4mTJ6s4TSI2Ux0zz1E99+vQWN2+OEHovBwHmVfu3bbrlOn+Do+/NDLPm7eJHr9dW6sVi0O2/KEf/7hiY///MdLg/QBTkb0roh8OIBDAKoCiAKww46QF7V53A7A8hz7FwL4XoReICIWj7vvJnrsMd272rmTqFEj/qS3aqXZHLB91q/njj76yPlxXbtyyN7Fi7qY0a0bT2Rq8sO2di1f09dfa9CYA+bOZR9Ny5a5fiArViR65hkv2j56lOi++/gaXn6Z6Pp172zt2pX9YufOedeODjgTeldcNw0BHCSiw0R0E8A8AO1z3BVctnlaCMCtZSxKqQ4ADgOQmjECoxS7b1av1j0HTK1a7CX65BOeB42PByZOdJga3zvGjuUVwL17Oz9u8GB2VUyfroMR2YnMHM0tusW0aby8tnNnDRpzwDPP8Gvx66/cj02crFcTsosXs6tm925OnjRlCi/c8oYRI9gv9umn3rVjhzNnuGldcPQLQNmj8ScBTLd5/hyASXaO6wce+R8HUM2yrRCADQAKA3gLDkb0AHoDSAKQdOedd/rk10/wMxs38ihr9myfdXn0KFGbNtxt8+Y8N6oZW7eSW4vBHnqIh6s3b2poBNGJExq5O4h41FqgANErr2jQmAtMmsTGd+58a2GZdY2Wg/la+6Sn88QxQFSvnvYr6jp0ICpRQtNZ4n/+IbrrLiKj0fM24KXr5ik7Qv+Zk+O7AphlefwBgM6Wxw6F3vZPXDf5hKwsXiXZsaNPuzWbiaZPZzd5nTosjJrw5JMcUeOqIi1ezF+/xESNDGAWLuRmN2zQoLFPPuHGtm3ToDEXef997rN7d6KsLFqxgp+uXu3i+YcOERkMfNKAAY7nSrzhr7+4/ffe06S5zZuJYmOJSpXy7n3zVujvA7DC5vkIACOcHB8G4JLl8ToA/1r+LgI4D6C/s/5E6PMR/fvzQhRXQ9w0ZPlyokKFeA3T/v1eNrZ3L/uYR450/ZysLJ7kNBg0myX++2+ie+/l35v0dC8bM5t50VeDBprY5hZvv83S1KcPnU01u66p33/PF1+8OE/y6kmLFrzYykuf/7Jl/DmsXNn7z6G3Qh8B9rFXQfZkbHyOY6rZPH7CXocyohdy8dtv/BFcuNAv3W/ezOlXSpYkMpm8aOj554kKFuR4c3f44gu+/t9/96JzZtUq9iaUKqVJc/yCAERTp2rQmJuYzdlRMoMHU5XKZnrqKSfHp6UR9e3LxzdqpPOMuwXrJPVnn3ncxMyZfGeZkKDNnaVXQs/now2Avy0++FGWbe8AaGd5/Al4snU7gDU5fwhIhF6wR0YGK1O3bn4z4eBBDgCKiWFvitscPszhgZ4sJrp2ja+/XTsPOs5m8mQ2IT6ezdGEF17goaamq5XcwGxm1wtA31cfTZUrOzjuwAH2wQFEQ4dqPufh1L4mTXiexc3bJ7OZ6N132eRHH9Vurshrofflnwh9PqNnT418DZ5z+jR7KMLCPBjA9ulDFBXFKy49YfRodvt4kBHy5k2eJwWIHn9cw8nlS5f4DuWllzRq0EOystgGgIbjXUpNzbF/zhwOdSxVimjpUt/b98sv/OJ/9ZXLp2RmZr9nXbtq+7EXoRcCl6VL+WO4bJlfzbhyhah1azblzTdddJunpLDI9+7tecenTnEbffu6ddq5cxw5BPD6HU2zH1tdSn/9pWGjHpKZSScf7UYE0N4+E3nbtWtEL77INj7wAKcn8AdmM0f1VKvm0huQlsZRNdabD2u+O60QoRcCl7Q0XjzUq5e/LaGbNzn9CcDm5ExVk4vBg9ln4kJ+daf07Mm+o7NnXTp83z52N0VF6bSOqW5ddof4ZClx3lw8m0ELYMz+FY6P57ugUaNceJN0ZsECciV66vx5TlyqFNHHH+tjigi9ENg8/TTPiOa6N/c9ZjPrB0D0xBO5VuVnk5rK7o1nn/W+0927uUMXqoQsX05UrBhPIv/5p/dd5yIpiW2ZNEmHxj0nvlo6bS5rWQRRpgzRr7/62yQmK4uoenVOreBgiH7sGGesjooi+u47/UwRoRcCm61b+VvQtq3297Me8vnnPPpq3NjB788bb/DXZ88ebTps1YrD9RzEfZvNRBMn8jxC7dpE//6rTbe5ePllvrtwa4WS/nTtSnR3hesc5aLZ4geN+OYb/iz89FOuXbt28XKRokU5yExPROiFwOezz/jj+P77/rbkFj/8wAtD77knR8TexYs8rPZmGWNOrIU9ZszItSs9nV1JAC/K1KyYSE6uXGE3WvfuOnXgOR99xNefR60S/5CRwXmxGza8zd21di1/TO64g2jHDv3NEKEXAh+zmYUzIsLLoHZtWbeO19+UK2ezQNQaG6dlOSuzmYfqNWveJhZnz2anTx8xQucbnunTuSNdfELe8ccfbJo/gmtcYsoUNnDlSiLitVtRUezV0e3uKwci9EJwcOECj4zuvDOgsgPu2cPh0kWKEP229BqvV2/VSvuOZs7kr+Ty5UTErvuqVfmu4ttvte8uF40asTIFyCSsLVeusCvtrbf8bYkDbtzg9NTNmtGnn7Kt99/v24+xM6GXwiNC4FC8OPDdd8DJk8ALL3DViQCgRg1gwwagcmVgSbtpQGoqMGqU9h098wxwxx3Ahx/i55+B++4Drl/nQirdumnf3W3s2gVs2sTFRTRJe6kthQsD1av7oFi4pxQoABoyFFi7FomvmtC+PddCL1nS34YxIvRCYNGgAfD++5xiduJEf1tziwoVgD9WpmNExPv4HU3x4aYHtO8kKgrUfwCwciVGPL4Ld9/NxaoaNdK+q1xMm8ZluZ5/3gedeYY1ZXGA/P7fRkYG0Gtzb6SiNL68810sWOB9RmQtEaEXAo9XXwU6dOByfH/95W9rblF88TeIvZmCdQ+OwtChnFbebNau/fR04NU9L+MaCmJipY+wbh1QsaJ27TskLQ2YPRvo1AkoVcoHHXqGwcBV/E6c8Lclt3PlCvD448BX8wphd/NBqHXsZ4Tv2u5vs27HkU/HX3/ioxeIiFeYVKrEf+fP+9sajqyoWpXIYKCsTDMNHMju9C5dtMmEe/o0L/IEiDY26EfmyEjfhRFawwP1jv/zks2b2cyKFYkGDuSoFk1XBHvAqVNE9evzurmvviKeZypalJxnYdMHyGSsEJRs3MhROB06+H+CcM4c/rosWkREbM6ECbzp4Ye9qwq4cyf/nkVHE82bR1yFwt20x97w4IO81Nbfr7ELzJ/PC9kKFODXPjaW0+H88os+qeed8fff/NtfsGCOaKCRI/n987Q2rYeI0AvBy4cf8sf0k0/8Z0NWFi+7j4/PFd84ezb/FtWuzalv3OWnnzgvV/nyPGK9RceOvFpY71z9+/aRlkU0fMXlyyz6Tz/N0VAA/3/mGQ5t1G2tgYVNm7jofOnS/Pg2zpxh9ffxegQReiF4MZt5CBcZmUMJfciiRfxVcRDj+OuvLNZ33sk1SFzBbCYaP54HfgaDneSXf/7JfU6e7J3teTF4MP9SnTqlbz86cuMG0c8/c56z0qX5ZYuOJmrfniNWtQ5x/OUX1vEqVXhUb5dBg9if44vc+BZE6IXg5tw5dsxWqeL7pflmMytx1apOE2ht2cIZDEqUIFq/3nmTaWlEzz3H376nn3ZQpMhs5pWWd9+tnyP6xg1O8dupkz7t+4GMDKI1aziVfVwcv8bh4Zz3/fPPvZ/2mDGD26tXL4/fxuPHeXDiZlZSbxChF4Ifk4lHnp06+daXbC1a6kKi+kOHOGNtdDTRjz/aP+bkSc6fAxC9804el/Ldd2Q7L6A58+Zx+ytW6NO+nzGbOdPy8OGcxoIDM4nuu48zbRw86F5bY8bw+S1auFiPpVcvnkzw0aS6CL0QGlhnP32ZWbFpU85K5eJM35kzPBAPC+NV8bZs28Y3JgULcnbbPMnI4FnaBx5w22yXaN6ci5UGSCI5PTGbeYXzmDGchdkq+nXq8GrbnTsd/+hmZmZXKnz2WTeKhRw8yB+EIUM0uw5niNALoUFWFlGbNpxEZMsW/ftbt46/IhMnunXa1auciBPgAlJmM5fFLViQ3Qlbt7rR2Mcfc0O5Zvy85OBBcjU1cihy+DAnSnvgAZ4nAdhLNmwY0YYN2b9916/zvDjAZWzdvpns1o1LMrpYa8AbROiF0CE1lUfYd92lYe08B7RqxfF7DpPSOyYjg+uJAFxa1Fq32u3si5cucVx2ly5u2+CU4cN5tOlpCcQQ4uRJoi+/JHrsMfYOAhwF1a8fv3dKEX36qYeNW2sNjB6tqc32EKEXQot163hGrHNn/fz11gIc777rcRNmM3+/AR7YpaV52NDQoXy9WqVBvHmTZ469LEoeily4wCGzRiOn5Y+K4jBOr+jYkVOg6jwwEaEXQo9x4/jj+8UX+rRvNHIycW9WQllISfHy9+jYMR5qvvaa17YQESfaB4iWLNGmvRDl2jWNgrysg4Zx4zRozDHOhF7x/sDBYDBQUsCmqBMCBrMZaNsWWLMG2LgRSEjQru29e4H4eOCNN4AxY7Rr1xu6dQOWLAGOHweKFfOurdatOVvlv/8CERGamCfkQatWwNat/JoXLKhLF0qpLURksLdPkpoJwUlYGPDNN5yEq3NnziylFePG8Zdx4EDt2vSWwYP5GqdP966do0eBFSuAF18Ukfclo0Zxemtv3z8PEaEXgpfYWCAxETh0CHj5ZW3y1x4+zG326QOULu19e1pRvz7w0EPAJ59wTlxPmTGD//fsqY1dgms8+CD/vf8+cPOmz7sXoReCm6ZNgXfeYXHWYrT03ntAeDgwZIj3bWnNkCHsulmwwLPzMzNZ6B97DKhUSVvbhLwZNQpITuY7UR8jPnoh+DGb2Qe6bh1XSapd27N2UlKAqlV5tPvFF9raqAVmM5dZKlqU8/S7Wwlq6VLgiSeAhQsBo1EfGwXHEHFhnYsXgf37NXediY9eCG3CwrhwRvHi7K+/etWzdj74AMjKAoYN09Y+rQgLA157jcssrVvn/vnTpgFly7LYC75HKR7VHzoEfP+9T7sWoRdCg7JlgblzgX/+Afr2dd9fn5oKfPklR7dUqaKPjVrw/PM8Af3hh+6dd+IE8PPPXIs3MlIf24S8ad+eI7refVfb8mR5IEIvhA4PPwy8+Sbw7bfA11+7d+7EicCNG8CIEfrYphUFCwKvvMKhln//7fp5X3/NdysvvaSfbULehIXxZ2z3bn4PfYT46IXQIisLaNkS2LCB/dg1a+Z9zsWLPDnZsqXPb6k94vRp4M47XZ9LMJuBu+7i+YfVq/W3T3BOZiZw7718Z7Zpk/tzLQ4QH72QfwgPB+bM4QnLzp2Ba9fyPmfyZODyZWDkSP3t04KyZYFnnwVmzQLOns37+FWreKFO7966mya4QEQEMHw4sHkzvzc+QIReCD3KlWOx378f6NfP+bHXrgEffwy0aQPUresb+7Rg8GAgLQ2YMiXvY6dN49Fjhw762yW4xvPPAxUqAGPH+qQ7EXohNGneHBg9mke9s2Y5Pm7qVODcOY6GCCbi4zmkdNIknltwxJkzwE8/Ad27AwUK+M4+wTkFCgBDhwK//w6sX697dyL0Qujy3/8CzZrx5OXevbn3p6dzSOVDDwH33+9z87xmyBD218+d6/iYWbN4Ja1MwgYevXrx6msfjOpdEnqlVCul1AGl1EGl1HA7+/sopXYppbYrpf5UStWwbG+hlNpi2bdFKfWI1hcgCA6x+usLFWJ//fXrt++fNYvDDoNtNG+leXNeHPbRR/bDSYnYbfPgg7zQSggsChXidRHLlnHCMz1xlNbS+gcgHMAhAFUBRAHYAaBGjmOK2jxuB2C55XFdAOUtj2sCSMmrP0lTLGjOihVcPaJnz+xtGRlc8LtBA9/WoNWamTM5Be7y5bn3rVnD+775xudmCS5y8SKnw9agQDucpCl2ZUTfEMBBIjpMRDcBzAPQPsePxWWbp4UAkGX7NiI6Ydm+B0C0UkochYJvadmSI2pmzOAVtAAwbx4nMBs1SrPwNr/wzDPAHXfYX0A1bRqvFn7ySd/bJbhGsWJA//7ADz8A+/bp1o0rQl8BwHGb58mWbbehlOqnlDoEYAKAV+200wnANiJKt3Nub6VUklIqKTU11TXLBcEd3nqLXRh9+7K/ftw4jrEP9nQAUVHAgAHAypXAzp3Z28+d45w2zz0HxMT4zz4hbwYN4vdo3DjdunBF6O0Nd3I5BIloMhHdBeB1AG/c1oBS8QDeA/CyvQ6IaCoRGYjIEBsb64JJguAmERGc4TImhjNe7t3Lo/ywEIhHePllXjH70UfZ22bP5snmXr38Z5fgGqVL83s4dy7fZeqAK5/yZAAVbZ7HATjh4FiAXTu3AnaVUnEAFgF4nogOeWKkIGhChQosgOfOAXffzRO0oUDJkpzDZu5c4OTJ7EnYRo2AWrX8bZ3gCkOHcvDAhAm6NO+K0G8GUE0pVUUpFQXgaQCLbQ9QSlWzedoWwD+W7cUB/AxgBBHpHywqCHnRqhWwaBGnOggP97c12jFoEC+tnzSJ0z/s3Suj+WCifHmuq9C8uS7Nu5TrRinVBsBEcATODCIaq5R6BzzLu1gp9QmARwFkALgAoD8R7VFKvQFgBCzCb6ElEZ1x1JfkuhEEDzEagbVrgUcfBZYv59DRwoX9bZXgI5zlupGkZoIQKvz5J084A+zzdSU9ghAySFIzQcgPNGkCNGzIj8VtI9ggZeAFIVRQCvjsM2DFCi4mLggWROgFIZRo2DB7VC8IFsR1IwiCEOKI0AuCIIQ4IvSCIAghjgi9IAhCiCNCLwiCEOKI0AuCIIQ4IvSCIAghjgi9IAhCiBNwuW6UUqkAjvrbDhcpDeCsv43QkVC+Prm24CWUr8+ba6tERHYLegSc0AcTSqkkR0mEQoFQvj65tuAllK9Pr2sT140gCEKII0IvCIIQ4ojQe8dUfxugM6F8fXJtwUsoX58u1yY+ekEQhBBHRvSCIAghjgi9IAhCiCNC7wZKqX+VUruUUtuVUkmWbSWVUiuVUv9Y/pfwt52uoJSaoZQ6o5TabbPN7rUo5lOl1EGl1E6lVD3/We4aDq7vLaVUiuX9224pem/dN8JyfQeUUo/5x2rXUEpVVEqtUUrtU0rtUUoNtGwP+vfPybUF/XunlIpWSv2llNphuba3LdurKKU2Wd6375RSUZbtBSzPD1r2V/a4cyKSPxf/APwLoHSObRMADLc8Hg7gPX/b6eK1NAVQD8DuvK4FQBsAywAoAI0BbPK3/R5e31sAhto5tgaAHQAKAKgC4BCAcH9fg5NruwNAPcvjIgD+tlxD0L9/Tq4t6N87y+tf2PI4EsAmy/sxH8DTlu1TAPS1PH4FwBTL46cBfOdp3zKi9572AGZZHs8C0MGPtrgMEf0B4HyOzY6upT2Ab4jZCKC4UuoO31jqGQ6uzxHtAcwjonQiOgLgIICArcdHRCeJaKvl8RUA+wBUQAi8f06uzRFB895ZXv+rlqeRlj8C8AiABZbtOd836/u5AEBzpZTypG8RevcgAL8qpbYopXpbtpUlopMAf0gBlPGbdd7j6FoqADhuc1wynH/5Apn+FvfFDBs3W9Ben+V2vi54dBhS71+OawNC4L1TSoUrpbYDOANgJfgO5CIRZVoOsbX/1rVZ9l8CUMqTfkXo3aMJEdUD0BpAP6VUU38b5CPsjSKCMS73CwB3AUgAcBLAh5btQXl9SqnCABYCGEREl50damdbQF+fnWsLifeOiLKIKAFAHPjOo7q9wyz/Nbs2EXo3IKITlv9nACwCv1GnrbfBlv9n/Geh1zi6lmQAq5qWMgAAAYxJREFUFW2OiwNwwse2eQ0RnbZ80cwApiH7Fj/ork8pFQkWwjlE9INlc0i8f/auLZTeOwAgoosA1oJ99MWVUhGWXbb237o2y/5icN0deRsi9C6ilCqklCpifQygJYDdABYD6G45rDuAn/xjoSY4upbFAJ63RG80BnDJ6iIIJnL4pTuC3z+Ar+9pS5RDFQDVAPzla/tcxeKn/QrAPiL6yGZX0L9/jq4tFN47pVSsUqq45XEMgEfBcxBrADxpOSzn+2Z9P58E8BtZZmbdxt8z0cHyB6AqeHZ/B4A9AEZZtpcCsBrAP5b/Jf1tq4vXkwi+Bc4AjxxedHQt4FvIyWB/4i4ABn/b7+H1zbbYv9PyJbrD5vhRlus7AKC1v+3P49oeAN/C7wSw3fLXJhTePyfXFvTvHYDaALZZrmE3gP9atlcF/zgdBPA9gAKW7dGW5wct+6t62rekQBAEQQhxxHUjCIIQ4ojQC4IghDgi9IIgCCGOCL0gCEKII0IvCIIQ4ojQC4IghDgi9IIgCCHO/wOEgr2L/YpXHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.plot(n_estimators, accuracy_train, '-b', label='acc_train')\n",
    "pylab.plot(n_estimators, accuracy_test, '-r', label='acc_test')\n",
    "pylab.legend(loc='upper left')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données non-équilibrées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "n_estimators = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    bagging = BaggingClassifier(base_estimator=MLPClassifier(hidden_layer_sizes=1, early_stopping = True, max_iter=300,learning_rate_init=0.1))\n",
    "    model = bagging.fit(X_train2, y_train2)\n",
    "    accuracy_train.append(bagging.score(X_train1, y_train1))\n",
    "    accuracy_test.append(bagging.score(X_val1, y_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.plot(n_estimators, accuracy_train, '-b', label='acc_train')\n",
    "pylab.plot(n_estimators, accuracy_test, '-r', label='acc_test')\n",
    "pylab.legend(loc='upper left')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 \n",
    "\n",
    "## Entrainement de forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"max_depth\": [1,2,3,4,5,6,7,8,9,10],\n",
    "    \"n_estimators\": [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(clf, param_dist) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données équilibrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10],\n",
       "                                        'n_estimators': [25, 50, 75, 100, 125,\n",
       "                                                         150, 175, 200, 225,\n",
       "                                                         250, 275, 300]})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809433962264151"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = search.best_estimator_\n",
    "model.fit(X_train1, y_train1)\n",
    "model.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(len(n_estimators)):\n",
    "    model = model.fit(X_train1, y_train1)\n",
    "    accuracy_train.append(model.score(X_train1, y_train1))\n",
    "    accuracy_test.append(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.plot(n_estimators, accuracy_train, '-b', label='acc_train')\n",
    "pylab.plot(n_estimators, accuracy_test, '-r', label='acc_test')\n",
    "pylab.legend(loc='upper left')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
